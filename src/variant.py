from utils.var_qc import get_var
from utils.data_file import filepath
from utils.var_qc import change_var
from utils.var_qc import concat_func
# from utils.var_qc import position
from utils.mkdir import mkdir
from utils.data_file import filepath
from utils.network import get_csv, urlencode
import pandas as pd
import numpy as np
from tqdm.notebook import tqdm
import os
from datetime import datetime
import time
from lxml import etree
from bs4 import BeautifulSoup as bs
import requests
import pandas
import json
from openpyxl import Workbook


# VARIANT data can be exported from: https://www.biosino.org/ViGTK/tools/varlist
# ARTICLE_GENOME is generated by the first step(genome.py)
# SARA2 & VAR_DOMAIN can support the findings of this study are available from the corresponding author upon reasonable request.

ROOT_DIR = os.path.dirname(os.path.abspath('code'))
# INPUT
VARIANT = filepath(ROOT_DIR, 'variant.csv')
ARTICLE_GENOME = filepath(ROOT_DIR, 'article', 'genome.tsv')
SARA2 = filepath(ROOT_DIR, 'sars2.xlsx')
PRO_DICT = filepath(ROOT_DIR, 'protein.tsv')
VAR_DOMAIN = filepath(ROOT_DIR, 'variant_domain.tsv')

# OUTPUT
OUTPUT_DIR = filepath(ROOT_DIR, 'article')
mkdir(OUTPUT_DIR)
NODE_VARIANT = filepath(OUTPUT_DIR, 'variant.tsv')



VAR_COLS = ['variant_id', 'genome_id', 'virus_id', 'taxon_id', 'position', 'ref', 'alt', 'type', 'region', 'gene',
            'var_aa', 'var_aa_pos', 'synonymous', 'mutation_frequency', 'protein', 'var_aa_id']


def build():
    mutation = pd.read_csv(
        VARIANT,
        encoding='GB2312',
        engine="python",
        error_bad_lines=False
    )

    genome = pd.read_csv(ARTICLE_GENOME, sep='\t',
                         encoding='utf-8', usecols=['virus_id', 'genome_id'])

    columns = {
        'accession_id': 'genome_id',
        '变异': 'variant_id',
        '物种编号': 'taxon_id',
        '染色体': 'chrom',
        '位置': 'position',
        'ref': 'ref',
        'alt': 'alt',
        '类型': 'type',
        '区域': 'region',
        '基因': 'gene',
        '氨基酸变化': 'var_aa',
        '氨基酸变化位置': 'var_aa_pos',
        '同义/非同义': 'synonymous',
        '宿主': 'hosts',
        '国家': 'countries',
        '突变频率': 'mutation_frequency',
        '基因组编号': 'virus_id',
        '毒株名称': 'virus_name',
        '宿主.1': 'host',
        '采样国家': 'country',
        '采样时间': 'collection_date',
        '蛋白': 'protein'
    }

    drop_columns = ['virus_name', 'host',
                    'country', 'collection_date', 'chrom']

    mutation = mutation.rename(columns=columns) \
        .drop(drop_columns, axis=1) \
        .drop_duplicates() \
        .assign(
        variant_id=lambda x: x.variant_id.apply(lambda x: x[12:]),
        var_aa=lambda x: x.var_aa.apply(get_var),
        var_aa_id=lambda x: x.apply(lambda x: change_var(
            x.synonymous, x.var_aa, x.var_aa_pos), axis=1)
    )
    print(mutation.protein.value_counts())

    mutation_merge = mutation.merge(genome, on='virus_id', how='left')
    mutation_drop = mutation_merge.dropna(subset=['genome_id'])
    variant_adjust = mutation_drop[VAR_COLS]
    print('success mutation file')

    sars_columns = {
        'primaryAccession': 'primary_accession',
        'Name': 'name',
        'Gene': 'gene',
        'Type': 'type',
        'Description': 'description',
        'Position': 'position',
        'evidenceCode': 'evidence_code'
    }

    use_columns = ['primaryAccession', 'Type', 'Description', 'Position']
    columns = ['primaryAccession', 'Name', 'Gene', 'Type', 'Description', 'location_start', 'location_end',
               'Position', 'evidenceCode', 'source', 'id', 'label', 'label_text', 'source_url']

    sars2 = pd.read_excel(
        SARA2,
        usecols=use_columns
    )\
        .assign(
        feature_id=lambda x: x.primaryAccession + ':' + x.Position
    ) \
        .rename(columns=sars_columns) \
        .drop_duplicates()

    feature_group = sars2.groupby(
        sars2['feature_id']).apply(concat_func).reset_index()
    feature_group_merge = feature_group.merge(sars2[['feature_id', 'primary_accession', 'position']], how='left', on='feature_id'). \
        drop_duplicates(subset='feature_id')[
        ['feature_id', 'type', 'description']]

    feature_rename = feature_group_merge.rename(
        columns={'type': 'domain_type'})
    print('success domain file')

    sars2 = pd .read_excel(
        SARA2
    ) \
        .drop_duplicates()

    VIGTK_ENTRY_MAP = {
        'R1A_SARS2': 'ORF1a_polyprotein',
        'SPIKE_SARS2': 'surface_glycoprotein',
        'R1AB_SARS2': 'ORF1ab_polyprotein',
        'AP3A_SARS2': 'ORF3a_protein',
        'VME1_SARS2': 'membrane_glycoprotein',
        'NS7A_SARS2': 'ORF7a_protein',
        'NCAP_SARS2': 'nucleocapsid_phosphoprotein',
        'VEMP_SARS2': 'envelope_protein',
        'NS6_SARS2': 'ORF6_protein',
        'ORF9B_SARS2': None,
        'A0A663DJA2_SARS2': 'ORF10_protein',
        'NS8_SARS2': 'ORF8_protein',
        'Y14_SARS2': None,
        'NS7B_SARS2': 'ORF7b_protein',
    }

    UNIPROT_COLUMN_MAP = {
        'Entry': 'entry',
        'Entry Name': 'entry_name',
        'Protein names': 'protein_names',
        'Gene Names': 'gene_names',
        'Organism': 'organism_name',
        'Organism (ID)': 'organism',
        'Binding site': 'binding_site',
        'Catalytic activity': 'catalytic_activity',
        'Domain [FT]': 'domain',
        'Region': 'region',
        'Motif': 'motif',
    }

    uniprot_csv_url = urlencode(
        'https://www.ebi.ac.uk/uniprot/api/covid-19/uniprotkb/stream', {
            'compressed': 'true',
            'format': 'tsv',
            'query': '*',  # AND (reviewed:true)
            'fields': ','.join([
                'accession', 'id', 'protein_name', 'gene_names', 'organism_name', 'organism_id',
                'ft_binding', 'cc_catalytic_activity', 'ft_domain', 'ft_region', 'ft_motif'
            ])
        })

    protein, _ = get_csv(uniprot_csv_url, sep='\t')
    protein.assign(
        vigtk_alias=lambda x: x['Entry Name'].map(VIGTK_ENTRY_MAP)
    ).rename(columns=UNIPROT_COLUMN_MAP) \
    .to_csv(
        PRO_DICT, sep='\t', index=False, encoding='utf-8')

    protein = pd.read_csv(
        PRO_DICT,
        sep='\t',
        encoding='utf-8',
        usecols=['entry', 'protein_names', 'vigtk_alias']
    )
    print('success protein file')
    columns_protein = {
        'entry': 'uniprot'
    }

    drop_columns = ['protein_split', 'vigtk_alias', 'protein']

    genome_variant_drop = variant_adjust[['protein', 'variant_id']].drop_duplicates() \
        .dropna(subset=['protein']) \
        .assign(
        variant_id=lambda x: x.variant_id.apply(lambda x: x[12:])
    )
    var_pro_split = genome_variant_drop['protein'].str.split(',', expand=True) \
        .stack() \
        .reset_index(level=1, drop=True) \
        .rename('protein_split')
    variant_protein = genome_variant_drop.join(var_pro_split) \
        .merge(protein[['entry', 'vigtk_alias']], left_on='protein_split', right_on='vigtk_alias', how='left') \
        .drop(drop_columns, axis=1) \
        .rename(columns=columns_protein)
    print('success mutation protein match')
    print(variant_protein.shape)
    feature_variant = pd.read_csv(
        VAR_DOMAIN,
        sep='\t',
        encoding='utf-8'
        )
    print('success mutation domain')
    feature_variant_match = feature_variant.merge(
        feature_rename, on='feature_id', how='left')
    feature_variant_merge = pd.merge(
        variant_adjust, feature_variant_match, on='variant_id', how='left').drop_duplicates()
    feature_variant_merge.rename(
        columns={'feature_id': 'domain_id'}, inplace=True)
    feature_variant_merge.to_csv(
        NODE_VARIANT,
        sep='\t',
        encoding='utf-8',
        index=False
    )
   
    print(feature_variant_merge.columns)
    print(feature_variant_merge.shape)
    print('success')


if __name__ == '__main__':
    build()
